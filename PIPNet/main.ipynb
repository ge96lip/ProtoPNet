{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce3f46",
   "metadata": {
    "lines_to_next_cell": 1
   },
   "outputs": [],
   "source": [
    "from pipnet.pipnet import PIPNet, get_network\n",
    "from util.log import Log\n",
    "import torch.nn as nn\n",
    "from util.args import get_args, save_args, get_optimizer_nn\n",
    "from util.data import get_dataloaders\n",
    "from util.func import init_weights_xavier\n",
    "from pipnet.train import train_pipnet\n",
    "from pipnet.test import eval_pipnet, get_thresholds, eval_ood\n",
    "from util.eval_cub_csv import eval_prototypes_cub_parts_csv, get_topk_cub, get_proto_patches_cub\n",
    "import torch\n",
    "from util.vis_pipnet import visualize, visualize_topk\n",
    "from util.visualize_prediction import vis_pred, vis_pred_experiments\n",
    "import sys, os\n",
    "import random\n",
    "import numpy as np\n",
    "from shutil import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e14533",
   "metadata": {},
   "source": [
    "hej"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8877492a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_pipnet(args=None):\n",
    "\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "\n",
    "    args = args or get_args()\n",
    "    assert args.batch_size > 1\n",
    "\n",
    "    # Create a logger\n",
    "    log = Log(args.log_dir)\n",
    "    print(\"Log dir: \", args.log_dir, flush=True)\n",
    "    # Log the run arguments\n",
    "    save_args(args, log.metadata_dir)\n",
    "    \n",
    "    gpu_list = args.gpu_ids.split(',')\n",
    "    device_ids = []\n",
    "    if args.gpu_ids!='':\n",
    "        for m in range(len(gpu_list)):\n",
    "            device_ids.append(int(gpu_list[m]))\n",
    "    \n",
    "    global device\n",
    "    if not args.disable_cuda and torch.cuda.is_available():\n",
    "        if len(device_ids)==1:\n",
    "            device = torch.device('cuda:{}'.format(args.gpu_ids))\n",
    "        elif len(device_ids)==0:\n",
    "            device = torch.device('cuda')\n",
    "            print(\"CUDA device set without id specification\", flush=True)\n",
    "            device_ids.append(torch.cuda.current_device())\n",
    "        else:\n",
    "            print(\"This code should work with multiple GPU's but we didn't test that, so we recommend to use only 1 GPU.\", flush=True)\n",
    "            device_str = ''\n",
    "            for d in device_ids:\n",
    "                device_str+=str(d)\n",
    "                device_str+=\",\"\n",
    "            device = torch.device('cuda:'+str(device_ids[0]))\n",
    "    else:\n",
    "        device = torch.device('cpu')\n",
    "     \n",
    "    # Log which device was actually used\n",
    "    print(\"Device used: \", device, \"with id\", device_ids, flush=True)\n",
    "    \n",
    "    # Obtain the dataset and dataloaders\n",
    "    trainloader, trainloader_pretraining, trainloader_normal, trainloader_normal_augment, projectloader, testloader, test_projectloader, classes = get_dataloaders(args, device)\n",
    "    if len(classes)<=20:\n",
    "        if args.validation_size == 0.:\n",
    "            print(\"Classes: \", testloader.dataset.class_to_idx, flush=True)\n",
    "        else:\n",
    "            print(\"Classes: \", str(classes), flush=True)\n",
    "    \n",
    "    # Create a convolutional network based on arguments and add 1x1 conv layer\n",
    "    feature_net, add_on_layers, pool_layer, classification_layer, num_prototypes = get_network(len(classes), args)\n",
    "   \n",
    "    # Create a PIP-Net\n",
    "    net = PIPNet(num_classes=len(classes),\n",
    "                    num_prototypes=num_prototypes,\n",
    "                    feature_net = feature_net,\n",
    "                    args = args,\n",
    "                    add_on_layers = add_on_layers,\n",
    "                    pool_layer = pool_layer,\n",
    "                    classification_layer = classification_layer\n",
    "                    )\n",
    "    net = net.to(device=device)\n",
    "    net = nn.DataParallel(net, device_ids = device_ids)    \n",
    "    \n",
    "    optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)   \n",
    "\n",
    "    # Initialize or load model\n",
    "    with torch.no_grad():\n",
    "        if args.state_dict_dir_net != '':\n",
    "            epoch = 0\n",
    "            checkpoint = torch.load(args.state_dict_dir_net,map_location=device)\n",
    "            net.load_state_dict(checkpoint['model_state_dict'],strict=True) \n",
    "            print(\"Pretrained network loaded\", flush=True)\n",
    "            net.module._multiplier.requires_grad = False\n",
    "            try:\n",
    "                optimizer_net.load_state_dict(checkpoint['optimizer_net_state_dict']) \n",
    "            except:\n",
    "                pass\n",
    "            if torch.mean(net.module._classification.weight).item() > 1.0 and torch.mean(net.module._classification.weight).item() < 3.0 and torch.count_nonzero(torch.relu(net.module._classification.weight-1e-5)).float().item() > 0.8*(num_prototypes*len(classes)): #assume that the linear classification layer is not yet trained (e.g. when loading a pretrained backbone only)\n",
    "                print(\"We assume that the classification layer is not yet trained. We re-initialize it...\", flush=True)\n",
    "                torch.nn.init.normal_(net.module._classification.weight, mean=1.0,std=0.1) \n",
    "                torch.nn.init.constant_(net.module._multiplier, val=2.)\n",
    "                print(\"Classification layer initialized with mean\", torch.mean(net.module._classification.weight).item(), flush=True)\n",
    "                if args.bias:\n",
    "                    torch.nn.init.constant_(net.module._classification.bias, val=0.)\n",
    "            # else: #uncomment these lines if you want to load the optimizer too\n",
    "            #     if 'optimizer_classifier_state_dict' in checkpoint.keys():\n",
    "            #         optimizer_classifier.load_state_dict(checkpoint['optimizer_classifier_state_dict'])\n",
    "            \n",
    "        else:\n",
    "            net.module._add_on.apply(init_weights_xavier)\n",
    "            torch.nn.init.normal_(net.module._classification.weight, mean=1.0,std=0.1) \n",
    "            if args.bias:\n",
    "                torch.nn.init.constant_(net.module._classification.bias, val=0.)\n",
    "            torch.nn.init.constant_(net.module._multiplier, val=2.)\n",
    "            net.module._multiplier.requires_grad = False\n",
    "\n",
    "            print(\"Classification layer initialized with mean\", torch.mean(net.module._classification.weight).item(), flush=True)\n",
    "    \n",
    "    # Define classification loss function and scheduler\n",
    "    criterion = nn.NLLLoss(reduction='mean').to(device)\n",
    "    scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader_pretraining)*args.epochs_pretrain, eta_min=args.lr_block/100., last_epoch=-1)\n",
    "\n",
    "    # Forward one batch through the backbone to get the latent output size\n",
    "    with torch.no_grad():\n",
    "        xs1, _, _ = next(iter(trainloader))\n",
    "        xs1 = xs1.to(device)\n",
    "        proto_features, _, _ = net(xs1)\n",
    "        wshape = proto_features.shape[-1]\n",
    "        args.wshape = wshape #needed for calculating image patch size\n",
    "        print(\"Output shape: \", proto_features.shape, flush=True)\n",
    "    \n",
    "    if net.module._num_classes == 2:\n",
    "        # Create a csv log for storing the test accuracy, F1-score, mean train accuracy and mean loss for each epoch\n",
    "        log.create_log('log_epoch_overview', 'epoch', 'test_top1_acc', 'test_f1', 'almost_sim_nonzeros', 'local_size_all_classes','almost_nonzeros_pooled', 'num_nonzero_prototypes', 'mean_train_acc', 'mean_train_loss_during_epoch')\n",
    "        print(\"Your dataset only has two classes. Is the number of samples per class similar? If the data is imbalanced, we recommend to use the --weighted_loss flag to account for the imbalance.\", flush=True)\n",
    "    else:\n",
    "        # Create a csv log for storing the test accuracy (top 1 and top 5), mean train accuracy and mean loss for each epoch\n",
    "        log.create_log('log_epoch_overview', 'epoch', 'test_top1_acc', 'test_top5_acc', 'almost_sim_nonzeros', 'local_size_all_classes','almost_nonzeros_pooled', 'num_nonzero_prototypes', 'mean_train_acc', 'mean_train_loss_during_epoch')\n",
    "    \n",
    "    \n",
    "    lrs_pretrain_net = []\n",
    "    # PRETRAINING PROTOTYPES PHASE\n",
    "    for epoch in range(1, args.epochs_pretrain+1):\n",
    "        for param in params_to_train:\n",
    "            param.requires_grad = True\n",
    "        for param in net.module._add_on.parameters():\n",
    "            param.requires_grad = True\n",
    "        for param in net.module._classification.parameters():\n",
    "            param.requires_grad = False\n",
    "        for param in params_to_freeze:\n",
    "            param.requires_grad = True # can be set to False when you want to freeze more layers\n",
    "        for param in params_backbone:\n",
    "            param.requires_grad = False #can be set to True when you want to train whole backbone (e.g. if dataset is very different from ImageNet)\n",
    "        \n",
    "        print(\"\\nPretrain Epoch\", epoch, \"with batch size\", trainloader_pretraining.batch_size, flush=True)\n",
    "        \n",
    "        # Pretrain prototypes\n",
    "        train_info = train_pipnet(net, trainloader_pretraining, optimizer_net, optimizer_classifier, scheduler_net, None, criterion, epoch, args.epochs_pretrain, device, pretrain=True, finetune=False)\n",
    "        lrs_pretrain_net+=train_info['lrs_net']\n",
    "        plt.clf()\n",
    "        plt.plot(lrs_pretrain_net)\n",
    "        plt.savefig(os.path.join(args.log_dir,'lr_pretrain_net.png'))\n",
    "        log.log_values('log_epoch_overview', epoch, \"n.a.\", \"n.a.\", \"n.a.\", \"n.a.\", \"n.a.\", \"n.a.\", \"n.a.\", train_info['loss'])\n",
    "    \n",
    "    if args.state_dict_dir_net == '':\n",
    "        net.eval()\n",
    "        torch.save({'model_state_dict': net.state_dict(), 'optimizer_net_state_dict': optimizer_net.state_dict()}, os.path.join(os.path.join(args.log_dir, 'checkpoints'), 'net_pretrained'))\n",
    "        net.train()\n",
    "    with torch.no_grad():\n",
    "        if 'convnext' in args.net and args.epochs_pretrain > 0:\n",
    "            topks = visualize_topk(net, projectloader, len(classes), device, 'visualised_pretrained_prototypes_topk', args)\n",
    "        \n",
    "    # SECOND TRAINING PHASE\n",
    "    # re-initialize optimizers and schedulers for second training phase\n",
    "    optimizer_net, optimizer_classifier, params_to_freeze, params_to_train, params_backbone = get_optimizer_nn(net, args)            \n",
    "    scheduler_net = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer_net, T_max=len(trainloader)*args.epochs, eta_min=args.lr_net/100.)\n",
    "    # scheduler for the classification layer is with restarts, such that the model can re-active zeroed-out prototypes. Hence an intuitive choice. \n",
    "    if args.epochs<=30:\n",
    "        scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=5, eta_min=0.001, T_mult=1, verbose=False)\n",
    "    else:\n",
    "        scheduler_classifier = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer_classifier, T_0=10, eta_min=0.001, T_mult=1, verbose=False)\n",
    "    for param in net.module.parameters():\n",
    "        param.requires_grad = False\n",
    "    for param in net.module._classification.parameters():\n",
    "        param.requires_grad = True\n",
    "    \n",
    "    frozen = True\n",
    "    lrs_net = []\n",
    "    lrs_classifier = []\n",
    "   \n",
    "    for epoch in range(1, args.epochs + 1):                      \n",
    "        epochs_to_finetune = 3 #during finetuning, only train classification layer and freeze rest. usually done for a few epochs (at least 1, more depends on size of dataset)\n",
    "        if epoch <= epochs_to_finetune and (args.epochs_pretrain > 0 or args.state_dict_dir_net != ''):\n",
    "            for param in net.module._add_on.parameters():\n",
    "                param.requires_grad = False\n",
    "            for param in params_to_train:\n",
    "                param.requires_grad = False\n",
    "            for param in params_to_freeze:\n",
    "                param.requires_grad = False\n",
    "            for param in params_backbone:\n",
    "                param.requires_grad = False\n",
    "            finetune = True\n",
    "        \n",
    "        else: \n",
    "            finetune=False          \n",
    "            if frozen:\n",
    "                # unfreeze backbone\n",
    "                if epoch>(args.freeze_epochs):\n",
    "                    for param in net.module._add_on.parameters():\n",
    "                        param.requires_grad = True\n",
    "                    for param in params_to_freeze:\n",
    "                        param.requires_grad = True\n",
    "                    for param in params_to_train:\n",
    "                        param.requires_grad = True\n",
    "                    for param in params_backbone:\n",
    "                        param.requires_grad = True   \n",
    "                    frozen = False\n",
    "                # freeze first layers of backbone, train rest\n",
    "                else:\n",
    "                    for param in params_to_freeze:\n",
    "                        param.requires_grad = True #Can be set to False if you want to train fewer layers of backbone\n",
    "                    for param in net.module._add_on.parameters():\n",
    "                        param.requires_grad = True\n",
    "                    for param in params_to_train:\n",
    "                        param.requires_grad = True\n",
    "                    for param in params_backbone:\n",
    "                        param.requires_grad = False\n",
    "        \n",
    "        print(\"\\n Epoch\", epoch, \"frozen:\", frozen, flush=True)            \n",
    "        if (epoch==args.epochs or epoch%30==0) and args.epochs>1:\n",
    "            # SET SMALL WEIGHTS TO ZERO\n",
    "            with torch.no_grad():\n",
    "                torch.set_printoptions(profile=\"full\")\n",
    "                net.module._classification.weight.copy_(torch.clamp(net.module._classification.weight.data - 0.001, min=0.)) \n",
    "                print(\"Classifier weights: \", net.module._classification.weight[net.module._classification.weight.nonzero(as_tuple=True)], (net.module._classification.weight[net.module._classification.weight.nonzero(as_tuple=True)]).shape, flush=True)\n",
    "                if args.bias:\n",
    "                    print(\"Classifier bias: \", net.module._classification.bias, flush=True)\n",
    "                torch.set_printoptions(profile=\"default\")\n",
    "\n",
    "        train_info = train_pipnet(net, trainloader, optimizer_net, optimizer_classifier, scheduler_net, scheduler_classifier, criterion, epoch, args.epochs, device, pretrain=False, finetune=finetune)\n",
    "        lrs_net+=train_info['lrs_net']\n",
    "        lrs_classifier+=train_info['lrs_class']\n",
    "        # Evaluate model\n",
    "        eval_info = eval_pipnet(net, testloader, epoch, device, log)\n",
    "        log.log_values('log_epoch_overview', epoch, eval_info['top1_accuracy'], eval_info['top5_accuracy'], eval_info['almost_sim_nonzeros'], eval_info['local_size_all_classes'], eval_info['almost_nonzeros'], eval_info['num non-zero prototypes'], train_info['train_accuracy'], train_info['loss'])\n",
    "            \n",
    "        with torch.no_grad():\n",
    "            net.eval()\n",
    "            torch.save({'model_state_dict': net.state_dict(), 'optimizer_net_state_dict': optimizer_net.state_dict(), 'optimizer_classifier_state_dict': optimizer_classifier.state_dict()}, os.path.join(os.path.join(args.log_dir, 'checkpoints'), 'net_trained'))\n",
    "\n",
    "            if epoch%30 == 0:\n",
    "                net.eval()\n",
    "                torch.save({'model_state_dict': net.state_dict(), 'optimizer_net_state_dict': optimizer_net.state_dict(), 'optimizer_classifier_state_dict': optimizer_classifier.state_dict()}, os.path.join(os.path.join(args.log_dir, 'checkpoints'), 'net_trained_%s'%str(epoch)))            \n",
    "        \n",
    "            # save learning rate in figure\n",
    "            plt.clf()\n",
    "            plt.plot(lrs_net)\n",
    "            plt.savefig(os.path.join(args.log_dir,'lr_net.png'))\n",
    "            plt.clf()\n",
    "            plt.plot(lrs_classifier)\n",
    "            plt.savefig(os.path.join(args.log_dir,'lr_class.png'))\n",
    "                \n",
    "    net.eval()\n",
    "    torch.save({'model_state_dict': net.state_dict(), 'optimizer_net_state_dict': optimizer_net.state_dict(), 'optimizer_classifier_state_dict': optimizer_classifier.state_dict()}, os.path.join(os.path.join(args.log_dir, 'checkpoints'), 'net_trained_last'))\n",
    "\n",
    "    topks = visualize_topk(net, projectloader, len(classes), device, 'visualised_prototypes_topk', args)\n",
    "    # set weights of prototypes that are never really found in projection set to 0\n",
    "    set_to_zero = []\n",
    "    if topks:\n",
    "        for prot in topks.keys():\n",
    "            found = False\n",
    "            for (i_id, score) in topks[prot]:\n",
    "                if score > 0.1:\n",
    "                    found = True\n",
    "            if not found:\n",
    "                torch.nn.init.zeros_(net.module._classification.weight[:,prot])\n",
    "                set_to_zero.append(prot)\n",
    "        print(\"Weights of prototypes\", set_to_zero, \"are set to zero because it is never detected with similarity>0.1 in the training set\", flush=True)\n",
    "        eval_info = eval_pipnet(net, testloader, \"notused\"+str(args.epochs), device, log)\n",
    "        log.log_values('log_epoch_overview', \"notused\"+str(args.epochs), eval_info['top1_accuracy'], eval_info['top5_accuracy'], eval_info['almost_sim_nonzeros'], eval_info['local_size_all_classes'], eval_info['almost_nonzeros'], eval_info['num non-zero prototypes'], \"n.a.\", \"n.a.\")\n",
    "\n",
    "    print(\"classifier weights: \", net.module._classification.weight, flush=True)\n",
    "    print(\"Classifier weights nonzero: \", net.module._classification.weight[net.module._classification.weight.nonzero(as_tuple=True)], (net.module._classification.weight[net.module._classification.weight.nonzero(as_tuple=True)]).shape, flush=True)\n",
    "    print(\"Classifier bias: \", net.module._classification.bias, flush=True)\n",
    "    # Print weights and relevant prototypes per class\n",
    "    for c in range(net.module._classification.weight.shape[0]):\n",
    "        relevant_ps = []\n",
    "        proto_weights = net.module._classification.weight[c,:]\n",
    "        for p in range(net.module._classification.weight.shape[1]):\n",
    "            if proto_weights[p]> 1e-3:\n",
    "                relevant_ps.append((p, proto_weights[p].item()))\n",
    "        if args.validation_size == 0.:\n",
    "            print(\"Class\", c, \"(\", list(testloader.dataset.class_to_idx.keys())[list(testloader.dataset.class_to_idx.values()).index(c)],\"):\",\"has\", len(relevant_ps),\"relevant prototypes: \", relevant_ps, flush=True)\n",
    "\n",
    "    # Evaluate prototype purity        \n",
    "    if args.dataset == 'CUB-200-2011':\n",
    "        projectset_img0_path = projectloader.dataset.samples[0][0]\n",
    "        project_path = os.path.split(os.path.split(projectset_img0_path)[0])[0].split(\"dataset\")[0]\n",
    "        parts_loc_path = os.path.join(project_path, \"parts/part_locs.txt\")\n",
    "        parts_name_path = os.path.join(project_path, \"parts/parts.txt\")\n",
    "        imgs_id_path = os.path.join(project_path, \"images.txt\")\n",
    "        cubthreshold = 0.5 \n",
    "\n",
    "        net.eval()\n",
    "        print(\"\\n\\nEvaluating cub prototypes for training set\", flush=True)        \n",
    "        csvfile_topk = get_topk_cub(net, projectloader, 10, 'train_'+str(epoch), device, args)\n",
    "        eval_prototypes_cub_parts_csv(csvfile_topk, parts_loc_path, parts_name_path, imgs_id_path, 'train_topk_'+str(epoch), args, log)\n",
    "        \n",
    "        csvfile_all = get_proto_patches_cub(net, projectloader, 'train_all_'+str(epoch), device, args, threshold=cubthreshold)\n",
    "        eval_prototypes_cub_parts_csv(csvfile_all, parts_loc_path, parts_name_path, imgs_id_path, 'train_all_thres'+str(cubthreshold)+'_'+str(epoch), args, log)\n",
    "        \n",
    "        print(\"\\n\\nEvaluating cub prototypes for test set\", flush=True)\n",
    "        csvfile_topk = get_topk_cub(net, test_projectloader, 10, 'test_'+str(epoch), device, args)\n",
    "        eval_prototypes_cub_parts_csv(csvfile_topk, parts_loc_path, parts_name_path, imgs_id_path, 'test_topk_'+str(epoch), args, log)\n",
    "        cubthreshold = 0.5\n",
    "        csvfile_all = get_proto_patches_cub(net, test_projectloader, 'test_'+str(epoch), device, args, threshold=cubthreshold)\n",
    "        eval_prototypes_cub_parts_csv(csvfile_all, parts_loc_path, parts_name_path, imgs_id_path, 'test_all_thres'+str(cubthreshold)+'_'+str(epoch), args, log)\n",
    "        \n",
    "    # visualize predictions \n",
    "    visualize(net, projectloader, len(classes), device, 'visualised_prototypes', args)\n",
    "    testset_img0_path = test_projectloader.dataset.samples[0][0]\n",
    "    test_path = os.path.split(os.path.split(testset_img0_path)[0])[0]\n",
    "    vis_pred(net, test_path, classes, device, args) \n",
    "    if args.extra_test_image_folder != '':\n",
    "        if os.path.exists(args.extra_test_image_folder):   \n",
    "            vis_pred_experiments(net, args.extra_test_image_folder, classes, device, args)\n",
    "\n",
    "\n",
    "    # EVALUATE OOD DETECTION\n",
    "    ood_datasets = [\"CARS\", \"CUB-200-2011\", \"pets\"]\n",
    "    for percent in [95.]:\n",
    "        print(\"\\nOOD Evaluation for epoch\", epoch,\"with percent of\", percent, flush=True)\n",
    "        _, _, _, class_thresholds = get_thresholds(net, testloader, epoch, device, percent, log)\n",
    "        print(\"Thresholds:\", class_thresholds, flush=True)\n",
    "        # Evaluate with in-distribution data\n",
    "        id_fraction = eval_ood(net, testloader, epoch, device, class_thresholds)\n",
    "        print(\"ID class threshold ID fraction (TPR) with percent\",percent,\":\", id_fraction, flush=True)\n",
    "        \n",
    "        # Evaluate with out-of-distribution data\n",
    "        for ood_dataset in ood_datasets:\n",
    "            if ood_dataset != args.dataset:\n",
    "                print(\"\\n OOD dataset: \", ood_dataset,flush=True)\n",
    "                ood_args = deepcopy(args)\n",
    "                ood_args.dataset = ood_dataset\n",
    "                _, _, _, _, _,ood_testloader, _, _ = get_dataloaders(ood_args, device)\n",
    "                \n",
    "                id_fraction = eval_ood(net, ood_testloader, epoch, device, class_thresholds)\n",
    "                print(args.dataset, \"- OOD\", ood_dataset, \"class threshold ID fraction (FPR) with percent\",percent,\":\", id_fraction, flush=True)                \n",
    "\n",
    "    print(\"Done!\", flush=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b07054",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    args = get_args()\n",
    "    torch.manual_seed(args.seed)\n",
    "    torch.cuda.manual_seed_all(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    print_dir = os.path.join(args.log_dir,'out.txt')\n",
    "    tqdm_dir = os.path.join(args.log_dir,'tqdm.txt')\n",
    "    if not os.path.isdir(args.log_dir):\n",
    "        os.mkdir(args.log_dir)\n",
    "    \n",
    "    sys.stdout.close()\n",
    "    sys.stderr.close()\n",
    "    sys.stdout = open(print_dir, 'w')\n",
    "    sys.stderr = open(tqdm_dir, 'w')\n",
    "    run_pipnet(args)\n",
    "    \n",
    "    sys.stdout.close()\n",
    "    sys.stderr.close()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
